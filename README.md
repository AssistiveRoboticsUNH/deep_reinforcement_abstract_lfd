# Deep Reinforcement Learning of Abstract Reasoning from Demonstrations (deep_reinforcement_abstract_lfd)
Madison Clark-Turner

The following repository contains a Deep Recurrent Q-Network (DRQN) architecture for the learning high-level human interactions from demonstrations (LfD). We specifically designed this network for use with an Applied Behavioral Analysis (ABA) styled social greeting behavioral intervention (BI). Our system collects demonstrations of the BI using a tele-operated robot and then extracts relevant features of the interaction in order to perform the intervention autonomously. The DRQN structure was designed using Tensorflow and integrated on a NAO humanoid robot using ROS. For the purposes of privacy we have excluded the raw demonstartion data but include the tools to generate additional demonstration files.

A description of our implementation along with both simulated results and live-system results are currently under review for presentation in [HRI 2018](http://humanrobotinteraction.org/2018/). 

Installation
=============

Get the appropriate files
Place files in the correct locations

Usage
=============

Usage of the system occurs in three steps:
-Collection of training data using a tele-operated robot as it delivers the desired BI
-Learning of the BI via the DRQN
-Execution of the learned BI using an autonomous system

Our implementation is designed for use with a social greeting BI. Which proceeds in the following manner:
1. The therapist/robot delivers a *Discriminitive Stimuli* (The robot says "hello" and waves)
2. The participant provides a *response* that is either compliant (responding to the robot) or non-compliant (refusing to acknowledge the robot's command)
3. The robot reacts to the participants response:
   - Compliant: the robot delivers a *reward* congratulating the participant on following the intervention. The BI then continues to step 
   - Non-compliant: the robot delivers a *prompt* instructing the participant to responsd in a compliant manner (saying "<Participant>, say hello" and waving). The BI then returns to step 2 or if several prompts have failed to elicit a compliant response then the BI proceeds tho step 4.
4. The robot ends the BI by saying "Good Bye"
 
Data Collection
--------------------

Data collection is performed using a tele-operated NAO humanoid robot. Demonstrations are first recorded as Rosbags and then later converted into TFRecords for training in the DRQN.

Operating the robot can be perfromed using our provided interface. The interface can be openned using the following commands in seperate terminals

```
roslaunch nao_bringup nao_full_py.launch
roslaunch deep_reinforcement_abstract_lfd interface.launch
```

The following interface will open

<interface picture>
  
The following buttons perfom the following operations:
-**Start:** Places the robot into a position it can deliever the BI by stiffening teh robot's joints and angelling the robot's head.
-**Shut Down:** delivers the *Discriminitive Stimuli*

-**Command:** delivers the *Discriminitive Stimuli*
-**Prompt:** delivers the *Discriminitive Stimuli*
-**Reward:** delivers the *Discriminitive Stimuli*
-**Abort:** delivers the *Discriminitive Stimuli*



Training the DRQN
--------------------




Execution of Autonomous System
--------------------

Data collection is performed by tele-operating the robot. 
In order to train the network you must list the directory containing traing files in the form of TFRecords in: X

While training the network our system will generate a checkpoint file every 10,000 iterations. You can train the model by executing:

```
python model_trainer.py
```

Evaluation of a model can be performed by running 
 
```
python evaluator.py
```

The live system can be run after a checkpoint file has been generated by executing:

```
  roslaunch nao_full_py.launch
  roslaunch dqn.launch
```

Dependencies
=============
The following libraries are used by this application:
- [Tensorflow 1.3.0](https://www.tensorflow.org/) - Deep network library
- [NumPy 1.13.3](http://www.numpy.org/), [SciPy 0.19.1](http://www.scipy.org/) - Python numerical libraries
- [OpenCV2 2.4.8](https://opencv.org/) - Open Source image processing library
- [Librosa 0.5.1](https://librosa.github.io/librosa/index.html) - Music and audio analysis library
- [ROS Indigo](http://wiki.ros.org/) - (Robot Operating System) Robotics library

Acknowledgents
=============

We borrowed code from several sources for this project:

- Spectral Subtraction: https://github.com/tracek/Ornithokrites.git
- Inception Network: https://github.com/tensorflow/models.git
